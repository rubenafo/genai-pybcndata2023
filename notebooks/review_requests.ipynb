{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208ce981-227a-4a03-9d76-0b1b17677dd2",
   "metadata": {},
   "source": [
    "# Workshop Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95bcbc-87a2-48a3-85f4-d73ef3952f00",
   "metadata": {},
   "source": [
    "## Retrieve HuggingFace API token\n",
    "1. Go to https://huggingface.co/\n",
    "2. Settings > Access Tokens > copy API token\n",
    "3. Update the token in notebooks/queries/access_token.txt\n",
    "\n",
    "## A word on the HF API rates\n",
    "- Limits are not disclosed, use API wisely.\n",
    "- Rates are reset every top of the hour.\n",
    "- Free tier should be enough to run this workshop.\n",
    "- Concurrent requests rates are much more strict than sequencial, avoid large request/sec.\n",
    "- You have room to modify the prompts and rerun anyway.\n",
    "\n",
    "## If you reach the rate limits\n",
    "- Workshop results are stored in user_info.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6cc606-9c18-47a7-aa2b-69374b220fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install requests\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505e37c-819a-4a4a-b157-e7e94216a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import loader\n",
    "from queries import Queries\n",
    "from queries import Prompts\n",
    "import pandas as pd\n",
    "\n",
    "# This is our email datasource\n",
    "emails_ds = loader.load_dataset()\n",
    "userinfo = pd.DataFrame(columns=[\"sentiment\", \"loan_qty\", \"sender\", \"motivation\", \"esg\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5758dca-1e19-47c7-9dd1-ce97e333347f",
   "metadata": {},
   "source": [
    "# The stuff ResponsibleLending has to deal with\n",
    "\n",
    "Take a look to the sample dataset of emails received at ResponsibleLending customer service center by running **view_dataset** notebook under this folder.    \n",
    "There is a mix of senders and motivations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f951c43-86a2-40df-abaa-1255874c42d1",
   "metadata": {},
   "source": [
    "# Exploring sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554244b2-f407-4731-9edf-27bd6a916150",
   "metadata": {},
   "source": [
    "LLM models are already pretrained with large corpus of data so some functionality is available out of the box.\n",
    "\n",
    "* In this example we'll be using Falcon-7B-instruct (https://huggingface.co/tiiuae/falcon-7b-instruct)\n",
    "* Falcon-40B is now available in HuggingFace Inference API: https://huggingface.co/tiiuae/falcon-40b\n",
    "\n",
    "If you want to try Falcon-40B you can do so by changing the model parameter in the run_query method:\n",
    "```python\n",
    "def run_query(payload={}, model=\"tiiuae/falcon-40b\"):\n",
    "```\n",
    "\n",
    "In the following example we'll classify email's sentiment in positive or negative using default model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f02cc3-fea3-4294-8ecf-ad079ce6d517",
   "metadata": {},
   "source": [
    "## Positive/negative classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a57bcf-a35c-498c-b998-fe25204d5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id,body in emails_ds.items():\n",
    "    prompt = Prompts.get_sentiment(body, \"positive\", \"negative\")\n",
    "    sentiment = Queries.run_query({\"inputs\": prompt})\n",
    "    print(f\"{id} -> {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c06bc2-d8fa-456d-8688-0ee8dfe57655",
   "metadata": {},
   "source": [
    "LLM models tend to be verbose as they are created to generate text. In order to simplify results processing it's useful to understand the parameters they use.\n",
    "\n",
    "Let's try the same query as before but now constraining the model a bit. Since we want to classify in two categories, we just need to generate one word (less tokens), plus we don't need the input text as part of the output.\n",
    "\n",
    "Limiting the number of generated tokens has two purposes:\n",
    "1. lower operational costs (less tokens generated)\n",
    "2. the output is easier to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cf857-d29e-48d7-b6ae-41ece80b3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id,body in emails_ds.items():\n",
    "    prompt = Prompts.get_sentiment(body, \"positive\", \"negative\")\n",
    "    sentiment = Queries.run_query({\"inputs\": prompt, \"parameters\":{\"max_new_tokens\": 3, \"return_full_text\": False}})\n",
    "    print(f\"{id} -> {sentiment}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573865b7-4c33-447f-80bc-3bc867aa66a5",
   "metadata": {},
   "source": [
    "Parameters change depending on the model and the execution environment.   \n",
    "HuggingFace Inference API uses different parameters depending on the task: https://huggingface.co/docs/api-inference/detailed_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac3c24-00f3-4236-acc4-8b6d26026b17",
   "metadata": {},
   "source": [
    "Note: Sentiment results above have a problem, most of the emails are still classified as positive!\n",
    "\n",
    "We need a better strategy to run the sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d30ae-eec1-4648-a774-e9ead2013d09",
   "metadata": {},
   "source": [
    "## A better classification?\n",
    "\n",
    "Maybe positive/negative is not the classification we are looking for..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf4434-c3e7-4d22-92c7-ea7469176fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id,body in emails_ds.items():\n",
    "    prompt = Prompts.get_sentiment(body, \"violent\", \"nonviolent\")\n",
    "    sentiment = Queries.run_query({\"inputs\": prompt, \"parameters\":{\"max_new_tokens\": 2, \"return_full_text\": False}})\n",
    "    sentiment = sentiment[0].get(\"generated_text\")\n",
    "    print(f\"{id} -> {sentiment}\")\n",
    "    userinfo.at[id, \"sentiment\"] = sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe98326-552a-4822-8ac6-46432da995fd",
   "metadata": {},
   "source": [
    "# Who's sending this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d35ac-04f3-4c87-976c-6757fbe9a3f2",
   "metadata": {},
   "source": [
    "Some models perform better than others for simple tasks.   \n",
    "Instead of falcon-7b-instruct let's use a very good zero-shot smaller model: flan-t5-xxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f6769-8b8f-4587-86c9-6ebb98d74a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id,body in emails_ds.items():\n",
    "    prompt = Prompts.get_sender(body)\n",
    "    sender = Queries.run_query({\"inputs\": prompt, \"parameters\":{\"max_new_tokens\": 12, \"return_full_text\": False}},\n",
    "                                 model=\"google/flan-t5-xxl\")\n",
    "    sender = sender[0].get(\"generated_text\")\n",
    "    print(f\"{id} -> {sender}\")\n",
    "    userinfo.at[id, \"sender\"] = sender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76022c-9f58-4653-895c-27580329036b",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. flan-t5-xxl is not even in the LLM top list anymore: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "2. Potential data leaks? Where is this LLM model running?\n",
    "3. Are larger models always more suitable than smaller ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802e6a4-036b-40bf-81ad-a17ce001ff7c",
   "metadata": {},
   "source": [
    "# How much are they asking for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbca1e-d938-45f1-b740-a5f176bed256",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id,body in emails_ds.items():\n",
    "    prompt = Prompts.get_loan(body)\n",
    "    loan = Queries.run_query({\"inputs\": prompt, \"parameters\":{\"max_new_tokens\": 10, \"return_full_text\": False}},\n",
    "                              model=\"google/flan-t5-xxl\")\n",
    "    loan = loan[0].get(\"generated_text\")\n",
    "    print(f\"{id} -> {loan}\")\n",
    "    userinfo.at[id, \"loan_qty\"] = loan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e8bff-00cb-484c-bdb9-06a6c9c4f843",
   "metadata": {},
   "source": [
    "Note: what happens if we use Falcon-7B-instruct model? (try just removing the model parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1e3f3-89e4-4107-a051-eec25dc3b286",
   "metadata": {},
   "source": [
    "# Fine tuning ...\n",
    "Fine tuning involves adjusting the _weights_ of the internal RNN of the LLM model (remember we mentioned _transformers?_)\n",
    "- benefits:\n",
    "    - more specific model\n",
    "    - faster\n",
    "- challenges:\n",
    "    - significant volume of training data\n",
    "    - model becomes less flexible\n",
    "    - computationally expensive, large GPUs\n",
    "    - specific expertise\n",
    " \n",
    "An example of a training dataset to customize Q/A chatbots: https://huggingface.co/datasets/samsum/viewer/samsum/\n",
    "\n",
    "## Think twice when fine tuning\n",
    "\n",
    "Is there any other alternative to fine tuning?\n",
    "LLM models are very flexible:\n",
    "1. prompt engineering: can't you just adjust the prompt?\n",
    "2. few shots training: provide examples to the LLM model as part of the context. Don't adjust the weights.\n",
    "3. enrich the context with large datasets: RAG approach: https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e3a33-3b32-421a-9b77-be1e5f9cb038",
   "metadata": {},
   "source": [
    "# ESG analysis\n",
    "\n",
    "In the following example we try to determine the motivation of the email sender.\n",
    "Very frequently one single model won't be the best approach for a specific scenario but a combination of them.\n",
    "\n",
    "In this case we use falcon-7b for the QA on the text generation and a custom model trained on ESG ranking for the text classification.\n",
    "\n",
    "The classification we are looking for, ESG (environment + social + governance), is very specific. There are some approaches:\n",
    "1. Zero-shot classification into ESG categories\n",
    "2. Enrich the context some examples related to ESG (few-shots classification)\n",
    "3. Fine tuning model: train a model with specific ESG data to customize the classification:  \n",
    "3.1 Use an existing model: https://huggingface.co/TrajanovRisto/bert-esg   \n",
    "3.2 Trained with a sample ESG dataset: https://huggingface.co/datasets/TrajanovRisto/esg-sentiment\n",
    "\n",
    "More on the LLM fine-tuning: T5 fine-tuning for Esperanto: https://github.com/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec9e5b-cddc-4325-b879-5e06c500820f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for id,body in emails_ds.items():\n",
    "    prompt = Prompts.get_purpose(body)\n",
    "    motivation = Queries.run_query({\"inputs\": prompt, \"parameters\":{\"max_new_tokens\": 100, \"return_full_text\": False}})\n",
    "    motivation = motivation[0].get(\"generated_text\")\n",
    "    print(f\"{id} -> {motivation}\\n\")\n",
    "    userinfo.at[id, \"motivation\"] = esg\n",
    "\n",
    "    esg = Queries.run_query({\"inputs\": Prompts.get_esg(motivation)}, model=\"TrajanovRisto/bert-esg\")\n",
    "    print(f\"{id} -> {esg}\")\n",
    "    esg = sorted(esg[0], key=lambda x: x['score'], reverse=True)[0:3]\n",
    "    esg = \"-\".join([l['label'] for l in esg])\n",
    "    print(f\"{id} -> {esg}\\n\")\n",
    "    userinfo.at[id, \"esg\"] = esg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84d8d0-a44d-48a0-a92b-b2277e5b3c67",
   "metadata": {},
   "source": [
    "# The secret sauce of ResponsibleLending \n",
    "\n",
    "Once we have defined the different dimensions for each customer it's time to build the loan recommendation system that will decide whether the sender gets the loan or not.   \n",
    "We use a mini-RAG approach, we enrich the context with some data to drive the model behaviour.   \n",
    "Let's use falcon-7b-instruct as model, we'll be generating the responses with a custom prompt.    \n",
    "Check _get_recommendation_ method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb50bb8-19ea-4aca-b838-87dd2b786abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load  user info from the snapshot\n",
    "#userinfo = pd.read_csv(\"userinfo.csv\", index_col=0)\n",
    "\n",
    "ids = list(userinfo.index)\n",
    "for id in ids:\n",
    "    sentiment = userinfo.at[id, \"sentiment\"]\n",
    "    loan_qty = userinfo.at[id, \"loan_qty\"]\n",
    "    sender = userinfo.at[id, \"sender\"]\n",
    "    motivation = userinfo.at[id, \"motivation\"]\n",
    "    esg_data = userinfo.at[id, \"esg\"]\n",
    "    prompt = Prompts.get_recommendation(sentiment, loan_qty, sender, motivation, esg_data)  \n",
    "    reply_email = Queries.run_query({\"inputs\": prompt, \"parameters\":{\"max_new_tokens\": 150, \"return_full_text\": False, \"temperature\":1}})\n",
    "\n",
    "    print(f\"\"\"{id} -> {reply_email}\\n\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945278d7-d147-4820-9785-7f4a136be72c",
   "metadata": {},
   "source": [
    "# Bonus section: How were the emails generated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903ad42-e468-4869-b46e-0527a2282bc1",
   "metadata": {},
   "source": [
    "## The villains\n",
    "Replace the sender with your favourite villain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc140ce-52fe-4edf-9802-817b35d9ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "You are Iceman. \n",
    "Introduce yourself, your residence and request a credit loan of $100000 to the bank's local branch of ResponsibleLending. \n",
    "You will use the money to defeat your greatest superhero rival.\n",
    "Provide details of your achievements and why you should get the loan.\n",
    "Mention the name of your greatest enemy, the reason why you are enemies and how you plan to eliminate him.\n",
    "Be informal.\n",
    "\n",
    "Email: Dear\"\"\"\n",
    "output = Queries.run_query({\"inputs\": query, \"parameters\": {\"max_new_tokens\": 900, \"return_full_text\" : False, \"temperature\":0.8}})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d135ca-35b1-495e-824a-299e3bbb568a",
   "metadata": {},
   "source": [
    "Note: notice how easy it is to add bias in LLM models, _eliminate him_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29c436-7da0-4ee2-9f48-1f89ac6ca1d5",
   "metadata": {},
   "source": [
    "## The superheros\n",
    "And the emails of the heros.   \n",
    "Replace it with the hero of your choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89b502-d7d3-4109-ae68-e28338c7e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "You are Batman.\n",
    "Write an email asking for $100000 to the local branch customer service of ResponsibleLending.\n",
    "Introduce yourself in great detail, including location of birth and current residence.\n",
    "Provide great detail of your well-known career achievements and concrete things you are famous for.\n",
    "Think of a new ESG-related project in your local community and describe it.\n",
    "\n",
    "Email: Dear\"\"\"\n",
    "output = Queries.run_query({\"inputs\": query, \"parameters\": {\"max_new_tokens\": 600, \"return_full_text\" : False, \"temperature\":0.8}})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5f7e4-debf-4ea7-83bf-8230347982a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
